{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to feed-forward neural networks\n",
        "\n",
        "---\n",
        "\n",
        "### Lecture: \"Physics-augmented machine learning\" @ Cyber-Physical Simulation, TU Darmstadt\n",
        "### Lecturer: Prof. Oliver Weeger\n",
        "### Content creators: Dominik K. Klein, Jasper O. Schommartz\n",
        "### Summer term 2024\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### In this notebook, you will...\n",
        "\n",
        "\n",
        "*   Calibrate feed-forward neural networks to different one-dimensional datasets\n",
        "*   Learn the influence of hyperparameters on the calibrated model\n",
        "* Learn the difference between interpolation and extrapolation\n",
        "*   Learn to construct convex and monotonous neural networks\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CAbS69NPtdTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Run the following cell to clone the GitHub repository in your current Google Colab environment.*"
      ],
      "metadata": {
        "id": "-Xin1T39xsu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/klein-tu-da/PAML_test.git"
      ],
      "metadata": {
        "id": "g_-FEL0BxvLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Run the following cell to import all modules and python files to this notebook. If you made changes in the python files, run the following cell again to update the python files in this notebook.*\n"
      ],
      "metadata": {
        "id": "y-v5FNcbWBpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import datetime\n",
        "now = datetime.datetime.now\n",
        "import PAML_test.FFNN_introduction.data as ld\n",
        "import PAML_test.FFNN_introduction.models as lm\n",
        "import PAML_test.FFNN_introduction.plots as lp"
      ],
      "metadata": {
        "id": "qFy4zsH0WAz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Nonlinear regression\n",
        "\n",
        "### Theory\n",
        "\n",
        "Introduce equation of FFNN. For arbitrary number of nodes and hidden layers. Introduce softplus, tanh, and linear activation function.\n",
        "\n",
        "$\\sigma(x)=...$\n",
        "\n",
        "Parameters of the FFNN are weights and bias. They are calibrated to fit the FFNN to a given dataset. By minimizing a loss function. Show the MSE. Introduce epochs. Iterative optimisation.\n",
        "\n",
        "FFNNs can be seen as very general ansatz functions. Thus, they can be applied to totally different datasets.\n",
        "\n",
        "\n",
        "### Task\n",
        "\n",
        "Calibrate a FFNN to different datasets. Vary the hyperparameters, i.e., the number of nodes, hidden layers, anc the activation function. Show a plot of all datasets that we use. Define variables up here.\n",
        "\n",
        "```\n",
        "units = [32,32,1]\n",
        "```\n",
        "\n",
        "changes the number of hidden layers and nodes.\n",
        "\n"
      ],
      "metadata": {
        "id": "skD3hX3uxA5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#   units: number of nodes in each hidden layer\n",
        "units = [32,32,1]\n",
        "\n",
        "#   acts: activation function in each hidden layer\n",
        "activation = ['softplus','softplus','linear']\n",
        "\n",
        "#   non_neg: restrict the weights in different layers to be non-negative\n",
        "non_neg = [False, True, True]\n",
        "\n",
        "#   data: 'bathtub', 'curve', 'double_curve\n",
        "data = 'curve'\n",
        "\n",
        "#   epochs: number of iterations in the optimisation process\n",
        "epochs = 100\n",
        "\n",
        " #   load model\n",
        " #  adapt this so that the students don't see the \"non_neg\" part at the first task.\n",
        "\n",
        "model = lm.main(units=units, activation=activation, non_neg=non_neg)\n",
        "\n",
        "#   load data\n",
        "\n",
        "xs, ys, xs_c, ys_c = ld.get_data(data)\n",
        "\n",
        "\n",
        "#   calibrate model\n",
        "\n",
        "t1 = now()\n",
        "print(t1)\n",
        "\n",
        "tf.keras.backend.set_value(model.optimizer.learning_rate, 0.002)\n",
        "h = model.fit([xs_c], [ys_c], epochs = epochs,  verbose = 2)\n",
        "\n",
        "t2 = now()\n",
        "print('it took', t2 - t1, '(sec) to calibrate the model')\n",
        "\n",
        "lp.plot_loss(h)\n",
        "\n",
        "\n",
        "# include the type of FFNN used. and show it in the plot title.\n",
        "lp.plot_data_model(xs, ys, xs_c, ys_c, model, data, 4)"
      ],
      "metadata": {
        "id": "19DfAdMzYFeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Convex and monotonous neural networks\n",
        "\n",
        "### Theory\n",
        "\n",
        "1D example for convexity and monotonicity. Write the conditions for convex and monotonous FFNNs down.\n",
        "\n",
        "Sufficient conditions for convex neural networks are\n",
        "\n",
        "\n",
        "*   A convex activation function in the first hidden layer\n",
        "*   Convex and non-decreasing activation functions in every subsequent layer\n",
        "* Non-negative weights in every layer beside the first one\n",
        "\n",
        "\n",
        "Sufficient conditions for monotonous neural networks are\n",
        "\n",
        "*  Monotonous activation functions in every layer\n",
        "* Non-negative weights in every layer\n",
        "* If at least one layer uses non-convex activation functions, the overall NN is not convex\n",
        "\n",
        "\n",
        "\n",
        "### Task\n",
        "\n",
        "Construct convex and monotonous FFNNs. Calibrate them to the datasets introduced above."
      ],
      "metadata": {
        "id": "li53gWrYxJwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#   units: number of nodes in each hidden layer\n",
        "units = [32,32,1]\n",
        "\n",
        "#   acts: activation function in each hidden layer\n",
        "activation = ['softplus','softplus','linear']\n",
        "\n",
        "#   non_neg: restrict the weights in different layers to be non-negative\n",
        "non_neg = [False, True, True]\n",
        "\n",
        "#   data: 'bathtub', 'curve', 'double_curve\n",
        "data = 'curve'\n",
        "\n",
        "#   epochs: number of iterations in the optimisation process\n",
        "epochs = 100\n",
        "\n",
        " #   load model\n",
        "\n",
        "model = lm.main(units=units, activation=activation, non_neg=non_neg)\n",
        "\n",
        "#   load data\n",
        "\n",
        "xs, ys, xs_c, ys_c = ld.get_data(data)\n",
        "\n",
        "\n",
        "#   calibrate model\n",
        "\n",
        "t1 = now()\n",
        "print(t1)\n",
        "\n",
        "tf.keras.backend.set_value(model.optimizer.learning_rate, 0.002)\n",
        "h = model.fit([xs_c], [ys_c], epochs = epochs,  verbose = 2)\n",
        "\n",
        "t2 = now()\n",
        "print('it took', t2 - t1, '(sec) to calibrate the model')\n",
        "\n",
        "lp.plot_loss(h)\n",
        "\n",
        "lp.plot_data_model(xs, ys, xs_c, ys_c, model, data, 4)"
      ],
      "metadata": {
        "id": "QBCtKDNwgjuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Sobolev training\n",
        "### Theory\n",
        "\n",
        "### Tasks"
      ],
      "metadata": {
        "id": "ZnvdLfoaZ9SO"
      }
    }
  ]
}